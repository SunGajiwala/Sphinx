<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />
<link rel="index" title="Index" href="genindex.html" /><link rel="search" title="Search" href="search.html" /><link rel="next" title="Understanding the Streamlit dashboard" href="Understanding_stream.html" /><link rel="prev" title="Overview" href="analyticsmain.html" />

    <!-- Generated with Sphinx 6.2.1 and Furo 2023.03.27 -->
        <title>Understanding the Notebooks - Admissions 0.0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/styles/furo.css?digest=fad236701ea90a88636c2a8c73b44ae642ed2a53" />
    <link rel="stylesheet" type="text/css" href="_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="index.html"><div class="brand">Admissions 0.0.1 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="index.html">
  
  
  <span class="sidebar-brand-text">Admissions 0.0.1 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Analytics and Dashboard</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="analyticsmain.html">Overview</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">Understanding the Notebooks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Machine Learning and Modelling</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Understanding_stream.html">Understanding the Streamlit dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="streamlit.html">Steps to run streamlit application</a></li>
<li class="toctree-l1"><a class="reference internal" href="sourcefile.html">Understanding functions of ADMS Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="model.html">Understanding the Classifier used</a></li>
<li class="toctree-l1"><a class="reference internal" href="retraining.html">Retraining the model</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="understanding-the-notebooks">
<h1>Understanding the Notebooks<a class="headerlink" href="#understanding-the-notebooks" title="Permalink to this heading">#</a></h1>
<ol class="arabic simple">
<li><p>First of all the codes have been merged with data preprocessing team so first copy and run the <a class="reference external" href="https://github.com/DSCI-Admissions-project/data-processing/blob/main/Data%20Preprocessing%20Updated.ipynbnotebooktogetthemergeddata.(ItincludesthefunctionalitiesfromData_Preprocessing_Analytics.ipynbandFeatureEngineering.ipynbsoyoudon’tneedtorunthemseparately">notebook</a></p></li>
<li><p>Run the Feature Selection.ipynb for the feature Selection process.</p></li>
</ol>
<p><strong>Power BI dashboard:</strong> <a class="reference external" href="https://app.powerbi.com/groups/me/reports/80cd2e24-4199-4c97-965b-87dedfadbf97/ReportSection?ctid=3c71cbab-b5ed-4f3b-ac0d-95509d6c0e93">Dashboard</a></p>
<section id="analysis-ipynb">
<h2>Analysis.ipynb<a class="headerlink" href="#analysis-ipynb" title="Permalink to this heading">#</a></h2>
<p>Preliminary Data <strong>Analysis.ipynb</strong> file has the initial Data analysis where we have used Statistical Techniques to:</p>
<ul class="simple">
<li><p>Measure central tendency (mean, median, mode) and dispersion (range, standard deviation, variance) for different columns</p></li>
<li><p>Frequency count for some categorical column</p></li>
</ul>
</section>
<section id="feature-engineering-ipynb">
<h2>Feature Engineering.ipynb<a class="headerlink" href="#feature-engineering-ipynb" title="Permalink to this heading">#</a></h2>
<p>In the Feature <strong>Engineering.ipynb</strong> we have engineered the following features:</p>
<ul class="simple">
<li><p>Max Job Number: The total number of jobs done by the applicant</p></li>
<li><p>Number of Degrees – the total number of degrees a person has</p></li>
<li><p>Has Masters or Doctoral Degree – if the person has masters or doctoral degree (1= True, 0= False)</p></li>
<li><p>latest_school_duration_in_year – The duration of their latest school</p></li>
<li><p>days_between_application_submitted_and_decision- days between application submitted and decision made (We can visualize this to see the timeframe of application since several years)</p></li>
<li><p>Is_Fresher : If the student has recently graduated or not</p></li>
<li><p>Total_experience_years : For some 58 rows, we are getting negative experiences, as looks like the start date is greater than the end date for them. Data processing team has been instructed to see if it is possible and reasonable to swap the start and end date while calculating months_of_experience(in months) . For now, carry on with what we have.</p></li>
<li><p>School_Gap_in_Years : The academic gap of student in years</p></li>
<li><p>Applying_while_in_school : See if the student is applying while in school ( 1=True, 0=False)</p></li>
<li><p>Gap_from_last_job : The number of years that represents the gap from last year. If Gap_from_last_job=0, then the applicants are employed during application.</p></li>
<li><p>Initial_Enrollment_from_Student : Since we don’t have enrolled column, this column is made from Applicantion I-20 sent date, such that it has a value of 1 if Application I-20 Sent Date is not null, -1 if the Admission Decision is “Denied” I.e. Not applicable, and 0 otherwise.</p></li>
<li><p>Standard test given: Whether GRE is given (1) or not(0)</p></li>
</ul>
<p>All the codes from feature Engineering have been added to the notebook from Data Preprocessing <a class="reference external" href="https://github.com/DSCI-Admissions-project/data-processing/blob/main/Data%20Preprocessing%20Updated.ipynb">LINK</a> for the one step processing of data to be ingested my Machine Learning Team. The documentation for the generation of these features is <a class="reference external" href="https://github.com/DSCI-Admissions-project/data-processing/blob/main/Documentation.pdf">here</a></p>
</section>
<section id="feature-selection-ipynb">
<h2>Feature Selection.ipynb<a class="headerlink" href="#feature-selection-ipynb" title="Permalink to this heading">#</a></h2>
<p>Let’s see the codes for the Feature Selection :</p>
<ol class="arabic" start="3">
<li><p>pd.read_csv(‘/Volumes/Admis-Shared/Visualization/Data_analytics_merged_data.csv’):</p>
<blockquote>
<div><p>This line of code reads the CSV file located at the path /Volumes/Admis-Shared/Visualization/Data_analytics_merged_data.csv using pandas and assigns it to a pandas DataFrame named merged_data_with_new_features.</p>
</div></blockquote>
</li>
<li><p>merged_data_with_new_features.drop(merged_data_with_new_features.columns[merged_data_with_new_features.columns.str.contains(‘Unnamed’,case=False)], axis=1, inplace=True):</p>
<blockquote>
<div><p>This line of code drops any columns from merged_data_with_new_features that contain the string ‘Unnamed’ in their column
name. The axis=1 argument specifies that columns should be dropped, and the inplace=True argument specifies that the DataFrame should be modified in place.</p>
</div></blockquote>
</li>
<li><p>Next, we drop multiple columns from df_final DataFrame, which will not be used in the model.</p></li>
<li><p>We divided the data by Application Area of Study to see the importance feature for different program which are named as df_DS fro Data Science and df_EM for Engineering Management.</p></li>
<li><p>corr_matrix_DS = df_DS.corr():</p>
<blockquote>
<div><p>This line of code calculates the correlation matrix for the df_DS DataFrame using the pandas corr() method and assigns it to the corr_matrix_DS variable and then we display it.</p>
</div></blockquote>
</li>
<li><p>We do the similar for Engineering Management.</p></li>
<li><p>Next, we have applied many different techniques such as :</p></li>
</ol>
<section id="recursive-feature-elimination">
<h3><strong>Recursive Feature Elimination.</strong><a class="headerlink" href="#recursive-feature-elimination" title="Permalink to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFE</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="c1"># replace null values with 0</span>
<span class="n">df_final</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df_DS</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Decision Name&#39;</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_DS</span><span class="p">[</span><span class="s1">&#39;Decision Name&#39;</span><span class="p">]</span>
<span class="c1"># create the RFE object with a logistic regression estimator and select the top 10 features</span>
<span class="n">rfe</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">),</span> <span class="n">n_features_to_select</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">rfe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># print the top 10 selected features</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;For Data Science: &quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Top 10 Features:&#39;</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">rfe</span><span class="o">.</span><span class="n">support_</span><span class="p">])</span>
</pre></div>
</div>
<p>The code provided is a Python script that performs feature selection on a dataset using the scikit-learn library. The script first imports the pandas library as pd and two classes from scikit-learn, RFE and LogisticRegression. It then replaces any null values in a dataframe, df_final, with 0 using the fillna method. The script creates two variables, X and y, by assigning the ‘Decision Name’ column of the dataframe to y and dropping the ‘Decision Name’ column from the dataframe and assigning the remaining columns to X.</p>
<p>Next, the script creates an instance of the RFE class, specifying a LogisticRegression estimator and selecting the top 15 features to retain. The fit method is then called on the RFE object, passing in X and y as arguments, to perform the feature selection. Finally, the script prints the top 10 selected features, as determined by the support attribute of the RFE object, to the
console. The output is labeled “For Data Science” and provides useful information for further analysis of the dataset.</p>
<p><strong>We did the same for the Engineering management data (df_EM) :</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFE</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="c1"># replace null values with 0</span>
<span class="n">df_final</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df_EM</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Decision Name&#39;</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_EM</span><span class="p">[</span><span class="s1">&#39;Decision Name&#39;</span><span class="p">]</span>

<span class="c1"># create the RFE object with a logistic regression estimator and select the top 10 features</span>
<span class="n">rfe</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">),</span> <span class="n">n_features_to_select</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">rfe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># print the top 10 selected features</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;For Engineering Management: &quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Top 10 Features:&#39;</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">rfe</span><span class="o">.</span><span class="n">support_</span><span class="p">])</span>
</pre></div>
</div>
<p><strong>Next, to test these selected features , we make a model with Logistic Regression to see the accuracy</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>
<span class="c1"># print the top 10 selected features print(&#39;-----------------&#39;)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;For Data Science&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Selected Features Using RFE:&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">rfe</span><span class="o">.</span><span class="n">support_</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df_DS</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Decision Name&#39;</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_DS</span><span class="p">[</span><span class="s1">&#39;Decision Name&#39;</span><span class="p">]</span>

<span class="c1"># create a new feature matrix with the selected features</span>
<span class="n">X_new</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">rfe</span><span class="o">.</span><span class="n">support_</span><span class="p">]]</span>
<span class="c1"># split the data into training and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_new</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># train a logistic regression model on the selected features</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># make predictions on the test set</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="c1"># evaluate the performance of the model</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Classification Report:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</pre></div>
</div>
<p>This code performs a classification task on a dataset called ‘df_DS’. First, the top 15 features are selected using recursive feature elimination (RFE) with a logistic regression estimator. Then, the dataset is split into training and test sets using the train_test_split function from sklearn. The logistic regression model is trained on the selected features using the training set, and then used to predict the class labels of the test set. The performance of the model is evaluated using the classification_report function from sklearn.metrics, which outputs precision, recall, F1-score, and support for each class label. The results of the evaluation are printed to the console. Overall, this code demonstrates how to perform a classification task using RFE for feature selection and logistic regression for modeling, and how to evaluate the performance of the model using the classification report.</p>
</section>
<section id="chi-square-test">
<h3><strong>Chi_Square Test :</strong><a class="headerlink" href="#chi-square-test" title="Permalink to this heading">#</a></h3>
<p>The code snippet performs a chi-square test to select the top 15 features for a logistic regression model. The code imports the chi2 function from the feature_selection module of the sklearn library. The dataset used for analysis is for data science. The dataset is loaded into a variable named ‘df_DS’. The dataset is preprocessed to remove the ‘Decision Name’ column from the feature matrix X, which is the predictor variables matrix. The target variable ‘Decision Name’ is assigned to the variable y.</p>
<p>The chi-square scores and p-values are calculated using the chi2 function with the feature matrix X and target variable y. The chi-square scores are then sorted in descending order, and the top 15 features are selected for the logistic regression model.</p>
<p>A new feature matrix is created using the selected top 15 features, and the data is split into training and test sets. A logistic regression model is then trained on the selected features, and predictions are made on the test set. Finally, the performance of the model is evaluated using the classification_report function from the metrics module of the sklearn library, which prints a report containing the precision, recall, f1-score, and support for each class.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">chi2</span>
<span class="c1"># Calculate chi-square statistics and p-values print (&#39;-----------&#39;)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Data Science&#39;</span><span class="p">)</span>

<span class="c1"># Fit the Random Forest model</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df_DS</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Decision Name&#39;</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_DS</span><span class="p">[</span><span class="s1">&#39;Decision Name&#39;</span><span class="p">]</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">chi_scores</span><span class="p">,</span> <span class="n">p_values</span> <span class="o">=</span> <span class="n">chi2</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Create a dataframe to store feature names and chi-square scores</span>
<span class="n">scores_df_chi_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Feature&#39;</span><span class="p">:</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="s1">&#39;Chi-Square Score&#39;</span><span class="p">:</span> <span class="n">chi_scores</span><span class="p">})</span>

<span class="c1"># Sort the dataframe by chi-square score in descending order and print the top 10 features</span>
<span class="n">top_10_features</span> <span class="o">=</span> <span class="n">scores_df_chi_test</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;Chi-Square Score&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">15</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">top_10_features</span><span class="p">)</span>

<span class="c1"># Select the top features</span>
<span class="n">top_features</span> <span class="o">=</span> <span class="n">scores_df_chi_test</span><span class="p">[</span><span class="s1">&#39;Feature&#39;</span><span class="p">][:</span><span class="mi">15</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

<span class="c1"># print the top 15 selected features</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Selected Top 15 Features using Chi_square:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">top_features</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">((</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">top_features</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

<span class="c1"># Create a new feature matrix with the selected features</span>
<span class="n">X_new</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">top_features</span><span class="p">]</span>
<span class="c1"># Split the data into training and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_new</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Train a logistic regression model on the selected features</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Make predictions on the test set</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="c1"># Evaluate the performance of the model</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Classification Report:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="random-forest-feature-importance">
<h3><strong>Random Forest Feature Importance:</strong><a class="headerlink" href="#random-forest-feature-importance" title="Permalink to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="c1"># Fit the Random Forest model</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df_EM</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Decision Name&#39;</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_EM</span><span class="p">[</span><span class="s1">&#39;Decision Name&#39;</span><span class="p">]</span>
<span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Get feature importance scores</span>
<span class="n">importance_scores</span> <span class="o">=</span> <span class="n">rf</span><span class="o">.</span><span class="n">feature_importances_</span>

<span class="c1"># Create a DataFrame to store the feature importance scores</span>
<span class="n">importance_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Feature&#39;</span><span class="p">:</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="s1">&#39;Importance&#39;</span><span class="p">:</span> <span class="n">importance_scores</span><span class="p">})</span>
<span class="c1"># Sort the features by their importance scores in descending order</span>
<span class="n">importance_df</span> <span class="o">=</span> <span class="n">importance_df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;Importance&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># Select the top features</span>
<span class="n">top_features</span> <span class="o">=</span> <span class="n">importance_df</span><span class="p">[</span><span class="s1">&#39;Feature&#39;</span><span class="p">][:</span><span class="mi">15</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-----------------&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;For Engineering Management &#39;</span><span class="p">)</span>
<span class="c1"># print the top 15 selected features</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Selected Top 15 Features using Random Forest Feature Importance:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">importance_df</span><span class="p">[:</span><span class="mi">15</span><span class="p">])</span>
<span class="c1"># Create a new feature matrix with the selected features</span>
<span class="n">X_new</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">top_features</span><span class="p">]</span>

<span class="c1"># Split the data into training and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_new</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="c1"># Train a logistic regression model on the selected features</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="c1"># Make predictions on the test set</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="c1"># Evaluate the performance of the model</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Classification Report:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</pre></div>
</div>
<p>This code is performing feature selection for the Engineering Management dataset (we also did for Data Science dataset) using the Random Forest feature importance method.
First, the code defines the predictor variables (X) and the response variable (y).
Then, a Random Forest Regressor model is fit on the data with 100 trees and a random seed of 42.
The feature importance scores are then calculated using the <strong>feature_importances_</strong> attribute of the model.
These scores are stored in a DataFrame with the corresponding feature names.
The DataFrame is sorted by importance scores in descending order, and the top 15 features are selected.
The selected features are printed, and a new feature matrix is created with the top 15 features.
The data is split into training and testing sets using a 80/20 ratio, and a logistic regression model is trained on the selected features.
The model is then used to make predictions on the test set, and the performance of the model is evaluated using the <strong>classification_report</strong> function from the <strong>sklearn.metrics</strong> module.</p>
<p>Here, among all, Random Forest Feature Importance has the best accuracy so we will be using this method for the feature selection .</p>
<p>Therefore, we again dropped some more columns from df_DS and df_EM which are not that important and have redundancy in information to generate our final feature list.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df_DS</span><span class="o">=</span> <span class="n">df_DS</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;TOEFL Reading&#39;</span><span class="p">,</span>
<span class="s1">&#39;TOEFL Writing&#39;</span> <span class="p">,</span> <span class="s1">&#39;TOEFL Listening&#39;</span><span class="p">,</span>
<span class="s1">&#39;TOEFL Speaking&#39;</span> <span class="p">,</span> <span class="s1">&#39;IELTS Reading&#39;</span> <span class="p">,</span>
<span class="s1">&#39;IELTS Writing&#39;</span> <span class="p">,</span> <span class="s1">&#39;IELTS Listening&#39;</span><span class="p">,</span>
<span class="s1">&#39;IELTS Speaking&#39;</span><span class="p">,</span> <span class="s1">&#39;TOEFL Reading&#39;</span><span class="p">,</span>
<span class="s1">&#39;TOEFL Writing&#39;</span><span class="p">,</span> <span class="s1">&#39;TOEFL Listening&#39;</span><span class="p">,</span>
<span class="s1">&#39;TOEFL Speaking&#39;</span><span class="p">,</span> <span class="s1">&#39;School GPA_1&#39;</span><span class="p">,</span>
<span class="s1">&#39;School GPA_2&#39;</span><span class="p">,</span> <span class="s1">&#39;School GPA_3&#39;</span><span class="p">,</span>
<span class="s1">&#39;School GPA_4&#39;</span><span class="p">,</span> <span class="s1">&#39;Job Organization_2&#39;</span><span class="p">,</span>
<span class="s1">&#39;Job Organization_3&#39;</span><span class="p">,</span> <span class="s1">&#39;Job Organization_4&#39;</span><span class="p">,</span>
<span class="s1">&#39;Job Organization_5&#39;</span><span class="p">,</span> <span class="s1">&#39;Job Organization_6&#39;</span><span class="p">,</span>
<span class="s1">&#39;Job Organization_7&#39;</span><span class="p">,</span> <span class="s1">&#39;Job Organization_8&#39;</span><span class="p">,</span>
<span class="s1">&#39;Job Organization_9&#39;</span><span class="p">,</span> <span class="s1">&#39;Duolingo Literacy&#39;</span><span class="p">,</span>
<span class="s1">&#39;Duolingo Comprehension&#39;</span><span class="p">,</span> <span class="s1">&#39;Duolingo Conversation&#39;</span><span class="p">,</span>
<span class="s1">&#39;Written_TOEFL Listening Comprehension&#39;</span><span class="p">,</span> <span class="s1">&#39;Written_TOEFL Reading Comprehension&#39;</span><span class="p">,</span>
<span class="s1">&#39;Written_TOEFL Structure/Written Expression&#39;</span><span class="p">,</span> <span class="s1">&#39;Written_TOEFL Test of Written English&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>And applied Random Forest Feature Importance to generate the top 10 features and the accuracy metrics.</p>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">#</a></h2>
<p>So, based on the analysis, the top 10 Features using Random Forest Feature Importance were:</p>
<p><strong>Feature Importance</strong>
1. School GPA (Recalculated)_1 –&gt; 0.182289</p>
<ol class="arabic simple" start="2">
<li><p>Application Area of Study –&gt; 0.136539</p></li>
<li><p>School Backlogs_1 –&gt; 0.102131</p></li>
<li><p>Job Organization_1 –&gt; 0.061276</p></li>
<li><p>Total_experience_years –&gt; 0.059822</p></li>
<li><p>Application Term of Entry –&gt; 0.059276</p></li>
<li><p>School Major_1 –&gt; 0.058107</p></li>
<li><p>latest_school_duration_in_year –&gt; 0.056608</p></li>
<li><p>Converted English Prof Score –&gt; 0.051250</p></li>
<li><p>School_Gap_in_Years –&gt; 0.050931</p></li>
<li><p>GRE Quantitative –&gt; 0.025911</p></li>
<li><p>GRE Verbal –&gt; 0.021301</p></li>
<li><p>Gap_from_last_job –&gt; 0.020369</p></li>
</ol>
<p><strong>Understanding the selected features:</strong></p>
<p><strong>School GPA (Recalculated)_1 :</strong> Contains the recalculated GPA for the latest degree ( can be masters or undergrad)</p>
<p><strong>Application Area of Study :</strong> The area of study</p>
<p><strong>School Backlogs_1 :</strong> Contains the backlog for the latest school of the students</p>
<p><strong>Application Term of Entry:</strong> The term of entry for admission ( Eg: Fall 2020, Spring 2021)</p>
<p><strong>latest_school_duration_in_year :</strong> It contains the total years of school attended</p>
<p><strong>Job Organization_1 :</strong> The latest employer of the students</p>
<p><strong>Total_experience_years :</strong> The total years of experience</p>
<p><strong>School_Gap_in_Years :</strong> The total gap between their school finished date and the application submitted date</p>
<p><strong>Converted English Prof Score :</strong> It is the converted english proficiency test(every test converted to IELTS)</p>
<p><strong>GRE Quantitative :</strong> Quantitative Score of GRE</p>
<p><strong>GRE Verbal :</strong> Verbal Score of GRE</p>
<p><strong>Gap_from_last_job:</strong> The total gap between the last job and application submitted date.</p>
<p>We obtained these features by employing the Random Forest Feature Importance method, which observes the correlation from the data. While developing the model, we suggest considering other features as well, as we may be excluding relevant information.</p>
<p>We also suggest considering the following additional features: Instead of using GRE Quantitative and GRE Verbal, we can try using Standardized test given.</p>
<ol class="arabic simple">
<li><p>Number of Degrees : The total number of degrees a student has.</p></li>
<li><p>Has Masters or Doctoral Degree : Whether the student has masters or Doctoral degree</p></li>
<li><p>Standardized test given : 1 for Standard Test Given and 0 for Not Given</p></li>
</ol>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="Understanding_stream.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Understanding the Streamlit dashboard</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="analyticsmain.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Overview</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2023, Sun Gajiwala
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Understanding the Notebooks</a><ul>
<li><a class="reference internal" href="#analysis-ipynb">Analysis.ipynb</a></li>
<li><a class="reference internal" href="#feature-engineering-ipynb">Feature Engineering.ipynb</a></li>
<li><a class="reference internal" href="#feature-selection-ipynb">Feature Selection.ipynb</a><ul>
<li><a class="reference internal" href="#recursive-feature-elimination"><strong>Recursive Feature Elimination.</strong></a></li>
<li><a class="reference internal" href="#chi-square-test"><strong>Chi_Square Test :</strong></a></li>
<li><a class="reference internal" href="#random-forest-feature-importance"><strong>Random Forest Feature Importance:</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/scripts/furo.js"></script>
    </body>
</html>